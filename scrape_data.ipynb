{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6c6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import json\n",
    "\n",
    "from urllib.parse import parse_qs, urlparse, urlsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65154aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_URL = \"https://www.gunviolencearchive.org\"\n",
    "QUERY_URL = \"https://www.gunviolencearchive.org/query\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "FORMAT = \"%m/%d/%Y\"\n",
    "QUERY_SEPERATOR = \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92ffbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_url(driver, start_date, end_date):\n",
    "    driver.get(QUERY_URL)\n",
    "    \n",
    "    # click add a rule\n",
    "    add_rule_button = driver.find_element(By.CSS_SELECTOR, '.filter-dropdown-trigger')\n",
    "    add_rule_button.click()\n",
    "    \n",
    "    # click date\n",
    "    date_filter = WebDriverWait(driver, 50).until(EC.element_to_be_clickable(driver.find_element(By.LINK_TEXT, 'Date')))\n",
    "    date_filter.click()\n",
    "    \n",
    "    # fill data fields\n",
    "    date_from = WebDriverWait(driver, 50).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[id*='filter-field-date-from']\")))\n",
    "    date_to = WebDriverWait(driver, 50).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[id*='filter-field-date-to']\")))\n",
    "    script = f\"arguments[0].setAttribute('value', '{start_date}');arguments[1].setAttribute('value', '{end_date}')\"\n",
    "    driver.execute_script(script, date_from, date_to)\n",
    "    \n",
    "    # click search\n",
    "    search_button = driver.find_element(By.ID, 'edit-actions-execute')\n",
    "    search_button.click()\n",
    "    \n",
    "    # sort date by ascending\n",
    "#     incident_date_a = driver.find_element(By.CSS_SELECTOR, 'a[title=\"sort by Incident Date\"]')\n",
    "#     incident_date_href = incident_date_a.get_attribute('href');\n",
    "#     driver.get(incident_date_href)\n",
    "    \n",
    "    return driver.current_url, get_n_pages(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db09dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_pages(driver):\n",
    "    try:\n",
    "        # get link related to last button\n",
    "        last_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[title=\"Go to last page\"]')))\n",
    "        last_url = last_button.get_attribute('href')\n",
    "        \n",
    "        # parse page number from page\n",
    "        form_data = urlparse(last_url).query\n",
    "        n_pages = int(parse_qs(form_data)['page'][0]) + 1\n",
    "\n",
    "        return n_pages\n",
    "    except NoSuchElementException:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4343d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(table_rows):\n",
    "    table = []\n",
    "    for tr in table_rows:\n",
    "        # find all elements in row\n",
    "        td = tr.find_all('td')\n",
    "        \n",
    "        # add all elements but last into list\n",
    "        row = [tr.text for tr in td[:(len(td)-1)]]\n",
    "        \n",
    "        # get incident url from last element in row\n",
    "        incident_url = DOMAIN_URL + td[len(td)-1].find(\"a\").get('href')\n",
    "        row.append(incident_url)\n",
    "        \n",
    "        # append to table\n",
    "        table.append(row)\n",
    "    return pd.DataFrame(table, columns=[\"id\", \"date\", \"state\", \"city\", \"address\", \"n_killed\", \"n_injured\", \"incident_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b83a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_query_url(url):\n",
    "    # request and open url + create beautifulsoup object for scraping\n",
    "    req = Request(url,headers=HEADERS)\n",
    "    query_page = urlopen(req)\n",
    "    soup = BeautifulSoup(query_page)\n",
    "    \n",
    "    # find table of page and find all table rows\n",
    "    res = soup.find_all('tbody')[0].find_all(\"tr\")\n",
    "    return convert_to_dataframe(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47279921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set driver to chrome\n",
    "driver = Chrome(executable_path=\"/Users/chasemattingly/chromedriver/chromedriver\")\n",
    "\n",
    "query_pairs = []\n",
    "step = timedelta(days=1)\n",
    "\n",
    "# change variables to change scraping range\n",
    "global_range = [\"1/1/2022\", \"8/1/2022\"]\n",
    "\n",
    "global_start_date = datetime.strptime(global_range[0], FORMAT)\n",
    "global_end_date = datetime.strptime(global_range[1], FORMAT)\n",
    "\n",
    "while global_start_date < global_end_date:\n",
    "    query_url, n_pages = get_query_url(driver, global_start_date.strftime(FORMAT), (global_start_date + step).strftime(FORMAT))\n",
    "    query_pairs.append((query_url, n_pages))\n",
    "    global_start_date += (step * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape stored queries for global date range\n",
    "df = pd.DataFrame()\n",
    "for url, n_pages in query_pairs:\n",
    "    print(url)\n",
    "    for page in range(n_pages):\n",
    "        # add page number to url\n",
    "        page_str = f'page={page}'\n",
    "        scrape_url = url + QUERY_SEPERATOR + page_str\n",
    "        \n",
    "        # scrape table and place into temp dataframe\n",
    "        scraped_df = scrape_query_url(scrape_url)\n",
    "        \n",
    "        # append scraped dataframe to global dataframe\n",
    "        df = df.append(scraped_df)\n",
    "df.to_csv('gun_violence_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine individual year dataframes \n",
    "year_range = [2018, 2022]\n",
    "combined_df = pd.DataFrame()\n",
    "for i in range(year_range[0], year_range[1]):\n",
    "    file_path = f\"data/gun_violence_{i}.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    combined_df = combined_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('gun_violence_2018-2021.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
